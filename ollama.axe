{
  "manifest": {
    "id": "ollama",
    "name": "Ollama (Local)",
    "type": "meta-service",
    "version": "1.0.0",
    "icon": "ðŸ¦™",
    "color": "#000000",
    "author": "Parachord Team",
    "description": "Run AI locally with Ollama. Free, private, works offline.",
    "homepage": "https://ollama.ai"
  },
  "capabilities": {
    "chat": true
  },
  "settings": {
    "requiresAuth": false,
    "authType": "none",
    "configurable": {
      "model": {
        "type": "select",
        "label": "Model",
        "description": "The Ollama model to use for chat",
        "options": [
          { "value": "llama3.1", "label": "Llama 3.1 (8B)" },
          { "value": "llama3.1:70b", "label": "Llama 3.1 (70B)" },
          { "value": "llama3.2", "label": "Llama 3.2 (3B)" },
          { "value": "mistral", "label": "Mistral (7B)" },
          { "value": "mixtral", "label": "Mixtral (8x7B)" },
          { "value": "qwen2.5", "label": "Qwen 2.5 (7B)" },
          { "value": "qwen2.5:14b", "label": "Qwen 2.5 (14B)" },
          { "value": "gemma2", "label": "Gemma 2 (9B)" }
        ],
        "default": "llama3.1"
      },
      "endpoint": {
        "type": "text",
        "label": "Ollama URL",
        "description": "URL where Ollama is running",
        "default": "http://localhost:11434",
        "placeholder": "http://localhost:11434"
      }
    }
  },
  "implementation": {
    "chat": "async function(messages, tools, config) {\n  const endpoint = config.endpoint || 'http://localhost:11434';\n  const model = config.model || 'llama3.1';\n\n  // Convert tools to Ollama format (same as OpenAI)\n  const ollamaTools = tools && tools.length > 0 ? tools.map(t => ({\n    type: 'function',\n    function: {\n      name: t.name,\n      description: t.description,\n      parameters: t.parameters\n    }\n  })) : undefined;\n\n  // Build request body\n  const body = {\n    model: model,\n    messages: messages,\n    stream: false\n  };\n\n  // Only include tools if we have them\n  if (ollamaTools && ollamaTools.length > 0) {\n    body.tools = ollamaTools;\n  }\n\n  const response = await fetch(`${endpoint}/api/chat`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify(body)\n  });\n\n  if (!response.ok) {\n    const text = await response.text();\n    throw new Error(`Ollama error (${response.status}): ${text}`);\n  }\n\n  const data = await response.json();\n\n  // Parse tool calls if present\n  let toolCalls = null;\n  if (data.message && data.message.tool_calls && data.message.tool_calls.length > 0) {\n    toolCalls = data.message.tool_calls.map(tc => {\n      // Ollama returns tool calls with function.name and function.arguments\n      const args = typeof tc.function.arguments === 'string'\n        ? JSON.parse(tc.function.arguments)\n        : tc.function.arguments;\n      return {\n        id: tc.id || `call_${Date.now()}_${Math.random().toString(36).slice(2, 9)}`,\n        name: tc.function.name,\n        arguments: args\n      };\n    });\n  }\n\n  return {\n    content: data.message?.content || '',\n    toolCalls: toolCalls\n  };\n}",
    "testConnection": "async function(config) {\n  const endpoint = config.endpoint || 'http://localhost:11434';\n  try {\n    const response = await fetch(`${endpoint}/api/tags`);\n    if (!response.ok) {\n      return { success: false, error: `HTTP ${response.status}` };\n    }\n    const data = await response.json();\n    const models = data.models || [];\n    return {\n      success: true,\n      models: models.map(m => m.name),\n      message: `Connected. ${models.length} model(s) available.`\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error: error.message || 'Connection failed'\n    };\n  }\n}"
  }
}
